{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:14.102214Z","iopub.status.busy":"2023-05-12T09:52:14.101850Z","iopub.status.idle":"2023-05-12T09:52:21.900048Z","shell.execute_reply":"2023-05-12T09:52:21.899085Z","shell.execute_reply.started":"2023-05-12T09:52:14.102185Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import cv2\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","# import matplotlib.pyplot as plt\n","# from detecto.utils import read_image \n","# from detecto.core import Dataset, Model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:22.037998Z","iopub.status.busy":"2023-05-12T09:52:22.035039Z","iopub.status.idle":"2023-05-12T09:52:22.042813Z","shell.execute_reply":"2023-05-12T09:52:22.041608Z","shell.execute_reply.started":"2023-05-12T09:52:22.037964Z"},"trusted":true},"outputs":[],"source":["train_dir = os.path.join('/kaggle/input/garbage-classification/Garbage classification/Garbage classification')\n","labels = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:23.634048Z","iopub.status.busy":"2023-05-12T09:52:23.633689Z","iopub.status.idle":"2023-05-12T09:52:24.020219Z","shell.execute_reply":"2023-05-12T09:52:24.019160Z","shell.execute_reply.started":"2023-05-12T09:52:23.634020Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2024 images belonging to 6 classes.\n","Found 503 images belonging to 6 classes.\n"]}],"source":["train_datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,\n","                                   rotation_range=15,zoom_range=0.1,\n","                                   width_shift_range=0.15,height_shift_range=0.15,\n","                                   shear_range=0.1,\n","                                   fill_mode=\"nearest\",\n","                                   rescale=1./255., \n","                                   validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir, target_size=(384, 512), batch_size=32, class_mode='binary', subset='training')\n","validation_generator = train_datagen.flow_from_directory(train_dir, target_size=(384, 512), batch_size=32, class_mode='binary', subset='validation')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:25.905640Z","iopub.status.busy":"2023-05-12T09:52:25.904776Z","iopub.status.idle":"2023-05-12T09:52:25.912005Z","shell.execute_reply":"2023-05-12T09:52:25.910775Z","shell.execute_reply.started":"2023-05-12T09:52:25.905602Z"},"trusted":true},"outputs":[],"source":["class myCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","        if(logs.get('accuracy')>0.90):\n","            print(\"\\nReached 90% accuracy so cancelling training!\")\n","            self.model.stop_training = True\n","\n","callbacks = myCallback()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:27.594091Z","iopub.status.busy":"2023-05-12T09:52:27.593740Z","iopub.status.idle":"2023-05-12T09:52:31.664528Z","shell.execute_reply":"2023-05-12T09:52:31.663579Z","shell.execute_reply.started":"2023-05-12T09:52:27.594063Z"},"trusted":true},"outputs":[],"source":["model = tf.keras.Sequential([\n","  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(384, 512, 3)),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Dropout(0.2),\n","    \n","  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Dropout(0.2),\n","    \n","  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Dropout(0.2),\n","\n","  tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Dropout(0.2),\n","    \n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(512, activation='relu'),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dropout(0.4),\n","  tf.keras.layers.Dense(6, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:31.666934Z","iopub.status.busy":"2023-05-12T09:52:31.666560Z","iopub.status.idle":"2023-05-12T09:52:31.688721Z","shell.execute_reply":"2023-05-12T09:52:31.687766Z","shell.execute_reply.started":"2023-05-12T09:52:31.666902Z"},"trusted":true},"outputs":[],"source":["model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(lr = 0.001), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T09:52:32.385471Z","iopub.status.busy":"2023-05-12T09:52:32.385115Z","iopub.status.idle":"2023-05-12T11:43:44.662261Z","shell.execute_reply":"2023-05-12T11:43:44.661239Z","shell.execute_reply.started":"2023-05-12T09:52:32.385426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["2023-05-12 09:52:36.878419: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["64/64 [==============================] - 175s 2s/step - loss: 1.9909 - accuracy: 0.4382 - val_loss: 3.0780 - val_accuracy: 0.1849\n","Epoch 2/50\n","64/64 [==============================] - 133s 2s/step - loss: 1.3540 - accuracy: 0.5380 - val_loss: 2.4756 - val_accuracy: 0.1869\n","Epoch 3/50\n","64/64 [==============================] - 133s 2s/step - loss: 1.2002 - accuracy: 0.5776 - val_loss: 1.9528 - val_accuracy: 0.2326\n","Epoch 4/50\n","64/64 [==============================] - 133s 2s/step - loss: 1.1239 - accuracy: 0.5914 - val_loss: 2.6400 - val_accuracy: 0.2406\n","Epoch 5/50\n","64/64 [==============================] - 132s 2s/step - loss: 1.0822 - accuracy: 0.6047 - val_loss: 2.7789 - val_accuracy: 0.2664\n","Epoch 6/50\n","64/64 [==============================] - 131s 2s/step - loss: 1.0105 - accuracy: 0.6240 - val_loss: 3.3303 - val_accuracy: 0.2247\n","Epoch 7/50\n","64/64 [==============================] - 131s 2s/step - loss: 1.0591 - accuracy: 0.6255 - val_loss: 4.2287 - val_accuracy: 0.2167\n","Epoch 8/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.9766 - accuracy: 0.6532 - val_loss: 2.3983 - val_accuracy: 0.3042\n","Epoch 9/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.9501 - accuracy: 0.6591 - val_loss: 2.3221 - val_accuracy: 0.3539\n","Epoch 10/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.9023 - accuracy: 0.6695 - val_loss: 3.3357 - val_accuracy: 0.2783\n","Epoch 11/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.8414 - accuracy: 0.6952 - val_loss: 2.3850 - val_accuracy: 0.3857\n","Epoch 12/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.8126 - accuracy: 0.6887 - val_loss: 1.8380 - val_accuracy: 0.4851\n","Epoch 13/50\n","64/64 [==============================] - 150s 2s/step - loss: 0.8424 - accuracy: 0.7055 - val_loss: 3.8021 - val_accuracy: 0.2266\n","Epoch 14/50\n","64/64 [==============================] - 133s 2s/step - loss: 0.8000 - accuracy: 0.7026 - val_loss: 2.0865 - val_accuracy: 0.4155\n","Epoch 15/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.7447 - accuracy: 0.7347 - val_loss: 1.5438 - val_accuracy: 0.5169\n","Epoch 16/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.7696 - accuracy: 0.7258 - val_loss: 1.2248 - val_accuracy: 0.5924\n","Epoch 17/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.7517 - accuracy: 0.7292 - val_loss: 2.2079 - val_accuracy: 0.3857\n","Epoch 18/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.7028 - accuracy: 0.7460 - val_loss: 2.8042 - val_accuracy: 0.3419\n","Epoch 19/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.6701 - accuracy: 0.7520 - val_loss: 3.1872 - val_accuracy: 0.4076\n","Epoch 20/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.6543 - accuracy: 0.7564 - val_loss: 2.6787 - val_accuracy: 0.3757\n","Epoch 21/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.6384 - accuracy: 0.7757 - val_loss: 1.1606 - val_accuracy: 0.6044\n","Epoch 22/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.6388 - accuracy: 0.7658 - val_loss: 1.6400 - val_accuracy: 0.5169\n","Epoch 23/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.6960 - accuracy: 0.7688 - val_loss: 1.8839 - val_accuracy: 0.4155\n","Epoch 24/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.6473 - accuracy: 0.7678 - val_loss: 2.2860 - val_accuracy: 0.3897\n","Epoch 25/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.5998 - accuracy: 0.7831 - val_loss: 4.6876 - val_accuracy: 0.2565\n","Epoch 26/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.5942 - accuracy: 0.7905 - val_loss: 3.6307 - val_accuracy: 0.3280\n","Epoch 27/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.5673 - accuracy: 0.8009 - val_loss: 2.0651 - val_accuracy: 0.4056\n","Epoch 28/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.6139 - accuracy: 0.7772 - val_loss: 1.5366 - val_accuracy: 0.5010\n","Epoch 29/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.5783 - accuracy: 0.7979 - val_loss: 1.3190 - val_accuracy: 0.5626\n","Epoch 30/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.5174 - accuracy: 0.8137 - val_loss: 1.6225 - val_accuracy: 0.5209\n","Epoch 31/50\n","64/64 [==============================] - 150s 2s/step - loss: 0.5150 - accuracy: 0.8187 - val_loss: 1.5594 - val_accuracy: 0.5109\n","Epoch 32/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.5499 - accuracy: 0.7974 - val_loss: 2.3398 - val_accuracy: 0.4453\n","Epoch 33/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.5635 - accuracy: 0.7935 - val_loss: 2.0403 - val_accuracy: 0.4533\n","Epoch 34/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.4946 - accuracy: 0.8236 - val_loss: 1.5451 - val_accuracy: 0.5885\n","Epoch 35/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.5063 - accuracy: 0.8291 - val_loss: 2.7469 - val_accuracy: 0.3837\n","Epoch 36/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.5471 - accuracy: 0.8014 - val_loss: 2.2011 - val_accuracy: 0.3936\n","Epoch 37/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.5047 - accuracy: 0.8132 - val_loss: 2.0802 - val_accuracy: 0.4871\n","Epoch 38/50\n","64/64 [==============================] - 149s 2s/step - loss: 0.5209 - accuracy: 0.8147 - val_loss: 2.4251 - val_accuracy: 0.4155\n","Epoch 39/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.4923 - accuracy: 0.8256 - val_loss: 1.5614 - val_accuracy: 0.5089\n","Epoch 40/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4469 - accuracy: 0.8394 - val_loss: 2.4806 - val_accuracy: 0.3857\n","Epoch 41/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.4802 - accuracy: 0.8286 - val_loss: 3.8187 - val_accuracy: 0.3002\n","Epoch 42/50\n","64/64 [==============================] - 130s 2s/step - loss: 0.4397 - accuracy: 0.8355 - val_loss: 2.4545 - val_accuracy: 0.4294\n","Epoch 43/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4024 - accuracy: 0.8557 - val_loss: 1.4543 - val_accuracy: 0.5885\n","Epoch 44/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4318 - accuracy: 0.8409 - val_loss: 1.3408 - val_accuracy: 0.6064\n","Epoch 45/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4345 - accuracy: 0.8434 - val_loss: 1.9486 - val_accuracy: 0.4871\n","Epoch 46/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4387 - accuracy: 0.8434 - val_loss: 3.3015 - val_accuracy: 0.3360\n","Epoch 47/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.4197 - accuracy: 0.8498 - val_loss: 6.4678 - val_accuracy: 0.2008\n","Epoch 48/50\n","64/64 [==============================] - 131s 2s/step - loss: 1.3096 - accuracy: 0.5830 - val_loss: 9.6381 - val_accuracy: 0.1829\n","Epoch 49/50\n","64/64 [==============================] - 132s 2s/step - loss: 0.9324 - accuracy: 0.6665 - val_loss: 4.9233 - val_accuracy: 0.2525\n","Epoch 50/50\n","64/64 [==============================] - 131s 2s/step - loss: 0.8149 - accuracy: 0.7001 - val_loss: 3.6983 - val_accuracy: 0.3280\n"]}],"source":["history = model.fit(train_generator, epochs=50, verbose=1, validation_data=validation_generator, callbacks=[callbacks])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T06:04:45.957565Z","iopub.status.busy":"2023-05-12T06:04:45.957223Z","iopub.status.idle":"2023-05-12T06:04:52.734550Z","shell.execute_reply":"2023-05-12T06:04:52.733573Z","shell.execute_reply.started":"2023-05-12T06:04:45.957534Z"},"trusted":true},"outputs":[],"source":["model.save('Litter Detection')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T12:53:45.018917Z","iopub.status.busy":"2023-05-12T12:53:45.018517Z","iopub.status.idle":"2023-05-12T12:53:47.677229Z","shell.execute_reply":"2023-05-12T12:53:47.676197Z","shell.execute_reply.started":"2023-05-12T12:53:45.018888Z"},"trusted":true},"outputs":[],"source":["model.save('model1.h5')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T12:01:03.035392Z","iopub.status.busy":"2023-05-12T12:01:03.035018Z","iopub.status.idle":"2023-05-12T12:01:06.700617Z","shell.execute_reply":"2023-05-12T12:01:06.699253Z","shell.execute_reply.started":"2023-05-12T12:01:03.035364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[ WARN:0] global /tmp/pip-req-build-4x5kub8r/opencv/modules/videoio/src/cap_v4l.cpp (890) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"]},{"ename":"TypeError","evalue":"load() missing 1 required positional argument: 'model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     camera\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     43\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(camera, class_names, confidence_threshold, model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_objects\u001b[39m(camera, class_names, confidence_threshold, model_path):\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mconf \u001b[38;5;241m=\u001b[39m confidence_threshold\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'model'"]}],"source":["import cv2\n","import torch\n","import numpy as np\n","\n","class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n","\n","# model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')\n","\n","camera = cv2.VideoCapture(0)\n","\n","def detect_objects(camera, class_names, confidence_threshold, model_path):\n","    model = torch.hub.load('custom', path=model_path)\n","    model.conf = confidence_threshold\n","    \n","    while True:\n","\n","        ret, frame = camera.read()\n","        \n","        with torch.no_grad():\n","            results = model(frame)\n","    \n","        predictions = results.pandas().xyxy[0]\n","        predictions = predictions[predictions['confidence'] >= confidence_threshold]\n","        labels = predictions['class'].astype(int).tolist()\n","        boxes = predictions[['xmin', 'ymin', 'xmax', 'ymax']].values.tolist()\n","        \n","        print(predictions)\n","        print(boxes)\n","        \n","        for label, box in zip(labels, boxes):\n","            if len(box) != 4:\n","                print(f\"Invalid box: {box}\")\n","                continue\n","            x_min, y_min, x_max, y_max = map(int, box)\n","            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n","            cv2.putText(frame, class_names[label], (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","            \n","        cv2.imshow('frame', frame)\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","    \n","    camera.release()\n","    cv2.destroyAllWindows()\n","\n","detect_objects(camera, class_names, confidence_threshold=0.5, model_path='/kaggle/working/model.pt')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
